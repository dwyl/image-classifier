{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of the models\n",
    "\n",
    "This notebook will create a table that compares different metrics between the models that have been benchmarked inside `coco_dataset`.\n",
    "To successfully run this notebook, it is advised to have a virtual `Conda` environment so this notebook has access to the needed dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing results files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = './coco_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_results_to_dataframe(directory_path=files_path):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    # Iterate over all files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('_results.csv'):\n",
    "            # Extract the model name from the filename\n",
    "            model_name = filename.replace('_results.csv', '')\n",
    "            \n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the expected columns are present in the DataFrame\n",
    "            if set(['image_id', 'time_in_microseconds', 'prediction']).issubset(df.columns):\n",
    "                # Add the model_name column\n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                # Keep only the required columns in the specified order\n",
    "                df = df[['image_id', 'model_name', 'time_in_microseconds', 'prediction']]\n",
    "                \n",
    "                # Append the dataframe to the list\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} does not contain the required columns.\")\n",
    "    \n",
    "    # Concatenate all dataframes in the list into a single dataframe\n",
    "    results_df = pd.concat(df_list)\n",
    "    \n",
    "    # Reset the index of the resulting dataframe\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function and assign the result to a variable\n",
    "df = load_results_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information from the COCO dataset captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the captions.csv file into a DataFrame\n",
    "captions_df = pd.read_csv(os.path.join(files_path, 'captions.csv'))\n",
    "\n",
    "# Rename the 'caption' column to 'original_caption'\n",
    "captions_df.rename(columns={'caption': 'original_caption'}, inplace=True)\n",
    "\n",
    "# Merge the two DataFrames on the 'image_id' column\n",
    "df = pd.merge(df, captions_df, on='image_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "Let's make both the `predicted caption` and the `original caption` lower case and formatted the same way so the metrics that we measure are more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the text according to the specified rules\n",
    "def clean_text(text):\n",
    "    # Make the text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove any surrounding quotation marks\n",
    "    text = text.strip('\\\"')\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove the period at the end if there is one\n",
    "    if text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'original_caption' and 'prediction' columns\n",
    "df['original_caption'] = df['original_caption'].apply(clean_text)\n",
    "df['prediction'] = df['prediction'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score\n",
    "\n",
    "Among the ensemble of evaluation metrics, the `ROUGE Score` is prominent. Standing for [\"Recall-Oriented Understudy for Gisting Evaluation\"](https://en.wikipedia.org/wiki/ROUGE_(metric)), the ROUGE Score is the lynchpin of automatic text summarization.\n",
    "\n",
    "The `ROUGE Score` has three main components: **`ROUGE-N`**, **`ROUGE-L`**, and **`ROUGE-S`**. \n",
    "\n",
    "Each ROUGE score component offers a different perspective on the quality of the system-generated summary, considering different aspects of language and sentence structure. This is why a combination of these measures is usually used in evaluating system outputs in NLP tasks.\n",
    "\n",
    "\n",
    "#### ROUGE-N\n",
    "`ROUGE-N` is a component of the ROUGE score that quantifies the overlap of [N-grams](https://en.wikipedia.org/wiki/N-gram) (contiguous sequences of N items - typically words or characters) between the system-generated summary and the reference summary. It provides insights into the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the system's output by considering the matching N-gram sequences.\n",
    "\n",
    "`ROUGE-N` essentially refers to the overlap of`n-grams`. It consists of `ROUGE-1` (overlap of **unigrams** - each word - between the system and reference summaries) and `ROUGE-2` (refers to the overlap of **bigrams** between the system and reference summaries).\n",
    "\n",
    "#### ROUGE-L \n",
    "`ROUGE-L`, another component of the `ROUGE Score`, calculates the [Longest Common Subsequence (LCS)](https://en.wikipedia.org/wiki/Longest_common_subsequence) between the system and reference summaries. Unlike N-grams, LCS measures the maximum sequence of words (not necessarily contiguous) that appear in both summaries. It offers a more flexible similarity measure and helps capture shared information beyond strict word-for-word matches.\n",
    "\n",
    "#### ROUGE-S\n",
    "`ROUGE-S` focuses on [skip-bigrams](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c). A skip-bigram is a pair of words in a sentence that allows for gaps or words in between. This component identifies the skip-bigram overlap between the system and reference summaries, enabling the assessment of sentence-level structure similarity. It can capture paraphrasing relationships between sentences and provide insights into the system's ability to convey information with flexible word ordering.\n",
    "\n",
    "> the text above was taken from https://thepythoncode.com/article/calculate-rouge-score-in-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `ROUGE Score` in our `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Function to calculate ROUGE scores for a single row\n",
    "def calculate_rouge_scores(row):\n",
    "    # Initialize the scorer for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores = scorer.score(row['original_caption'], row['prediction'])\n",
    "    \n",
    "    # Extract and return the scores\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "# Apply the calculate_rouge_scores function to each row in df\n",
    "# The result will be a new DataFrame with the ROUGE scores\n",
    "rouge_scores_df = df.apply(calculate_rouge_scores, axis=1, result_type='expand')\n",
    "\n",
    "# Concatenate the original df with the new DataFrame containing the ROUGE scores\n",
    "df = pd.concat([df, rouge_scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above will create three new columns `rouge1`, `rouge2` and `rougeL`. \n",
    "\n",
    "`rouge-scorer` returns a `Score` object with three different parameters. For example:\n",
    "\n",
    "```python\n",
    "rouge1:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.7142857142857143, recall=0.8333333333333334, fmeasure=0.7692307692307692)]\n",
    "rouge2:\n",
    "[Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272), Score(precision=0.3333333333333333, recall=0.4, fmeasure=0.3636363636363636)]\n",
    "rougeL:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)]\n",
    "```\n",
    "\n",
    "The choice between using `fmeasure`, `precision`, or `recall` depends on what aspect of the summary's quality you want to emphasize:\n",
    "\n",
    "- **Precision** (specificity) measures the fraction of relevant instances among the retrieved instances. In the context of ROUGE, it calculates how many of the words in the predicted summary (generated caption) are also found in the reference summary (original caption).\n",
    "\n",
    "- **Recall** measures the fraction of relevant instances that were retrieved. In the context of ROUGE, it calculates how much of the reference summary is captured by the predicted summary.\n",
    "\n",
    "- **F-measure**(or F1 score) is the harmonic mean of precision and recall. It provides a single score that balances both precision and recall. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "We are choosing the `F-measure` because it strikes a nice balance between `precision` and `recall`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU \n",
    "\n",
    "`BLEU` stands for [\"Bilingual Evaluation Understudy\"](https://en.wikipedia.org/wiki/BLEU) and it's a metric commonly used in NLP. IIt acn be used in text summarization, paraphrasing tasks and inclusively *image captioning*.\n",
    "\n",
    "The BLEU score is based on a simple idea, comparing the machine-generated translations with human-generated translations that are considered correct.\n",
    "Here's how it works:\n",
    "\n",
    "- The machine translation system generates translations for a set of sentences.\n",
    "- These machine-generated translations are compared to the reference translations.\n",
    "- The comparison is done by counting how many words or phrases from the machine-generated translations match the words or phrases in the reference translations.\n",
    "- The more matches there are, the higher the BLEU score will be.\n",
    "\n",
    "The BLEU score considers the precision of matching words or phrases. It also considers the length of the translations to avoid favoring shorter translations that may have an advantage in matching words by chance.\n",
    "\n",
    "The BLEU score is typically represented as a value between 0 and 1, with 1 being a perfect match and 0 being a perfect mismatch to the reference translations.\n",
    "\n",
    "> the text above was taken from https://thepythoncode.com/article/bleu-score-in-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `BLUE` score in our `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Function to calculate BLEU score for a single row\n",
    "def calculate_bleu_score(row):\n",
    "    # Prepare the reference and hypothesis\n",
    "    reference = [row['original_caption']]\n",
    "    hypothesis = row['prediction']\n",
    "    # Calculate BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu([hypothesis], [reference])\n",
    "    # Return the BLEU score\n",
    "    return bleu.score\n",
    "\n",
    "# Apply the calculate_bleu_score function to each row in df\n",
    "df['BLEU_score'] = df.apply(calculate_bleu_score, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes when interpreting the `BLEU` score\n",
    "\n",
    "The BLEU score ranges from 0 to 100:\n",
    "\n",
    "- `0` indicates a complete lack of overlap between the candidate translation (in your case, the predicted caption) and the reference translations (the original captions), which implies very poor quality.\n",
    "- `100` indicates a perfect match with the reference translations, signifying an ideal result.\n",
    "  \n",
    "In practice, you'll rarely see a `BLEU` score of `100`, especially in tasks other than translation, because it would require the candidate text to match the reference exactly, including word choice and order. \n",
    "Even human translators don't often achieve a perfect score because there are many possible ways to correctly translate or summarize a text!\n",
    "\n",
    "When interpreting `BLEU` scores, consider the following:\n",
    "\n",
    "- **Higher scores are better**, as they indicate more `n-gram` overlap with the reference text and, by extension, better quality text generation.\n",
    "- `BLEU` uses `n-gram precision`, which does not capture semantics or meaning. It only measures how many n-grams (up to a certain size) match between the candidate and the reference texts.\n",
    "- **`BLEU` is sensitive to the length of the text.** Very short or very long texts may produce misleading scores.\n",
    "- `BLEU` includes brevity penalty to penalize overly short generated text, as short candidates can have high precision by just including common n-grams.\n",
    "  \n",
    "\n",
    "In summary, a higher BLEU score *suggests better resemblance to the reference text at the surface level* (in terms of the exact words and their order), but it does not necessarily mean that the candidate text is more accurate or appropriate. \n",
    "\n",
    "You can find a small text by Google on how to interpret the `BLEU` score in https://cloud.google.com/translate/automl/docs/evaluate#bleu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEOR\n",
    "\n",
    "`METEOR` aka [\"Metric for Evaluation of Translation with Explicit Ordering\"](https://en.wikipedia.org/wiki/METEOR) is an automatic metric for evaluating machine translation output that addresses some of the shortcomings of the `BLEU` score. While `BLEU` focuses on precision by measuring how many words in the machine translation output appear in the reference translation, `METEOR` **also accounts for recall by considering how many words in the reference are captured in the translation**. \n",
    "\n",
    "Overall, METEOR is designed to correlate better with human judgment of translation quality than `BLEU`. \n",
    "It does this by considering a wider range of linguistic phenomena and by balancing precision and recall. Because it aligns words between the candidate and reference texts and accounts for synonyms and stemming, `METEOR` is often seen as providing a more nuanced evaluation of translation outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `METEOR` score in our `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Ensure that the Punkt tokenizer models are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate METEOR score for a single row\n",
    "def calculate_meteor(row):\n",
    "    # Assuming 'original_caption' is the reference and 'prediction' is the hypothesis\n",
    "    reference = row['original_caption']\n",
    "    hypothesis = row['prediction']\n",
    "    # Tokenize both the reference and the hypothesis\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    hypothesis_tokens = word_tokenize(hypothesis)\n",
    "    # Calculate the METEOR score\n",
    "    score = meteor_score([reference_tokens], hypothesis_tokens)\n",
    "    return score\n",
    "\n",
    "# Apply the calculate_meteor function to each row in df\n",
    "df['METEOR_score'] = df.apply(calculate_meteor, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the code above a bit further:\n",
    "\n",
    "- both the `reference` and the `hypothesis` are tokenized using `word_tokenize`.\n",
    "- the `meteor_score` function takes a list of **tokenized reference sentences** (even if there's only one reference) and a **tokenized hypothesis**.\n",
    "- tt calculates the `METEOR` score for each row and adds the scores to a new column named `'METEOR_score'` in `df` dataframe.\n",
    "\n",
    "Before all of this, we download [`punkt`](https://www.nltk.org/api/nltk.tokenize.punkt.html), a tokenizer model. It is used to divide a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Error Rate\n",
    "\n",
    "The [**Word Error Rate (WER)**](https://en.wikipedia.org/wiki/Word_error_rate) is a common metric for evaluating the performance of a speech recognition or machine translation system. It compares a reference text to a hypothesis text, and it is calculated as the number of substitutions, insertions, and deletions needed to change the hypothesis into the reference, divided by the number of words in the reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `Word Error Rate` in our `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "# Function to calculate WER for a single row\n",
    "def calculate_wer(row):\n",
    "    # Assuming 'original_caption' is the reference and 'prediction' is the hypothesis\n",
    "    reference = row['original_caption']\n",
    "    hypothesis = row['prediction']\n",
    "    # Calculate WER using jiwer\n",
    "    wer_score = jiwer.wer(reference, hypothesis)\n",
    "    return wer_score\n",
    "\n",
    "# Apply the calculate_wer function to each row in df\n",
    "df['Word_error_rate'] = df.apply(calculate_wer, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can interpret the WER score:\n",
    "\n",
    "- **`WER = 0`**: This means that the hypothesis (the generated text) matches the reference (the target text) perfectly. There are no errors at all.\n",
    "- **`0 < WER < 1`**: The hypothesis has errors, but the number of errors is less than the number of words in the reference. This indicates that there are some mistakes, but more than half of the words are correct.\n",
    "- **`WER = 1`**: The number of errors is equal to the number of words in the reference. This could mean that every word is wrong, or that the hypothesis is of the same length as the reference but completely different.\n",
    "- **`WER > 1`**: The hypothesis is so inaccurate that the number of errors exceeds the number of words in the reference. This can happen if the hypothesis is longer than the reference and contains many incorrect words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data \n",
    "\n",
    "Now that we have the scores of different metrics for each caption, it's time to aggregate the data!\n",
    "We are going to condense the data to have an evaluation of each model with the execution time and the precision/accuracy of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll perform aggregation for each `image_id`. Because there are 5 captions describing each image in the COCO Dataset, we are getting the **best results of each score for each image**. Since we have a level of redundancy when describing images, it's fair to give the *best score for the prediction at a given image* instead of the average of the scores of each image. \n",
    "\n",
    "This is what we're doing in the next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation dictionary for the scores\n",
    "aggregations = {\n",
    "    'rouge1': 'max',\n",
    "    'rouge2': 'max',\n",
    "    'rougeL': 'max',\n",
    "    'BLEU_score': 'max',\n",
    "    'METEOR_score': 'max',\n",
    "    'Word_error_rate': 'min' # the lower the error rate, the better\n",
    "}\n",
    "\n",
    "# Group by the specified columns and aggregate using the specified functions\n",
    "condensed_df = df.groupby(['image_id', 'model_name', 'time_in_microseconds', 'prediction']).agg(aggregations).reset_index()\n",
    "\n",
    "# The resulting condensed_df will have one row per group with the highest or minimum scores as specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can aggregate the **average** and the **median** of the scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation dictionary for calculating mean and median\n",
    "aggregations = {\n",
    "    'time_in_microseconds': ['mean', 'median'],\n",
    "    'rouge1': ['mean', 'median'],\n",
    "    'rouge2': ['mean', 'median'],\n",
    "    'rougeL': ['mean', 'median'],\n",
    "    'BLEU_score': ['mean', 'median'],\n",
    "    'METEOR_score': ['mean', 'median'],\n",
    "    'Word_error_rate': ['mean', 'median']\n",
    "}\n",
    "\n",
    "# Group by the model_name and calculate the specified aggregations\n",
    "final_df = condensed_df.groupby('model_name').agg(aggregations)\n",
    "\n",
    "# Flatten the MultiIndex columns by combining the level 0 and level 1 column names\n",
    "final_df.columns = ['_'.join(col).strip() for col in final_df.columns.values]\n",
    "\n",
    "# Reset the index to turn the model_name index back into a column\n",
    "final_df = final_df.reset_index()\n",
    "\n",
    "# The resulting final_df will have the average and median values for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the **median** and the **average** of each score for a given model.\n",
    "But, to make it simple for people to see, which one should we use if we had to? \n",
    "\n",
    "### Median or average?\n",
    "\n",
    "You don't *have* to read this small section. We're just doing this so we know we're providing statistically-correct results. \n",
    "\n",
    "We have chosen a sample size of **50 images** *on purpose* because of the [`Central Limit Theorem`](https://www.investopedia.com/terms/c/central_limit_theorem.asp). 30 samples is often used as a rule of thumb for a minimum sample size in statistics because it is the point at which this theorem begins to apply. The CLT states that **the distribution of sample means will be approximately normal, regardless of the distribution of the population from which the samples are drawn, as long as the sample size is large enough.**\n",
    "\n",
    "Conducting normality tests will help us decide if we can show the `average` or stick with `median`. Basically, if the data is **sufficiently uniform**, we can safely use the `average` aggregator. Otherwise, we should use `median` to filter out outliers. \n",
    "\n",
    "Because our sample size is `<= 50`, we can perform a [**Shapiro-Wilk Normality test**](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) to check for the distribution of our sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# List of the score columns to test for normality\n",
    "score_columns = ['time_in_microseconds', 'rouge1', 'rouge2', 'rougeL', 'BLEU_score', 'METEOR_score', 'Word_error_rate']\n",
    "\n",
    "# Perform the Shapiro-Wilk test for each score column\n",
    "for column in score_columns:\n",
    "    stat, p_value = stats.shapiro(condensed_df[column])\n",
    "    #print(f'Column: {column}')\n",
    "    #print('Test statistic:', stat)\n",
    "    #print('p-value:', p_value)\n",
    "\n",
    "    # Interpret the p-value\n",
    "    alpha = 0.05\n",
    "    if p_value > alpha:\n",
    "        print('')\n",
    "        #print('Sample looks Gaussian (fail to reject H0)\\n')\n",
    "    else:\n",
    "        print('')\n",
    "        #print('Sample does not look Gaussian (reject H0)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above for a single model yielded the following.\n",
    "\n",
    "```\n",
    "Column: time_in_microseconds\n",
    "Test statistic: 0.40061622858047485\n",
    "p-value: 5.369669552751644e-13\n",
    "Sample does not look Gaussian (reject H0)\n",
    "\n",
    "Column: rouge1\n",
    "Test statistic: 0.9838210344314575\n",
    "p-value: 0.7199809551239014\n",
    "Sample looks Gaussian (fail to reject H0)\n",
    "\n",
    "Column: rouge2\n",
    "Test statistic: 0.9751147031784058\n",
    "p-value: 0.3686423897743225\n",
    "Sample looks Gaussian (fail to reject H0)\n",
    "\n",
    "Column: rougeL\n",
    "Test statistic: 0.987608015537262\n",
    "p-value: 0.8752310872077942\n",
    "Sample looks Gaussian (fail to reject H0)\n",
    "\n",
    "Column: BLEU_score\n",
    "Test statistic: 0.8733219504356384\n",
    "p-value: 7.060639472911134e-05\n",
    "Sample does not look Gaussian (reject H0)\n",
    "\n",
    "Column: METEOR_score\n",
    "Test statistic: 0.987629771232605\n",
    "p-value: 0.8760008811950684\n",
    "Sample looks Gaussian (fail to reject H0)\n",
    "\n",
    "Column: Word_error_rate\n",
    "Test statistic: 0.9659003019332886\n",
    "p-value: 0.1569066196680069\n",
    "Sample looks Gaussian (fail to reject H0)\n",
    "```\n",
    "\n",
    "As you can see above, every column has a normal distribution **except the `BLEU` score column**.\n",
    "\n",
    "We could use the average/mean on all of these except `BLEU`. But, for simplicity sake, we'll use median on every single column.\n",
    "\n",
    "> **NOTE**:\n",
    ">\n",
    "> If you run this with multiple models, the distribution might be different. This test should be done **for each model**, not with the dataframe that has the results for multiple models. \n",
    "> You don't need to worry about this though, we are sticking with the `median` regardless. The output above pertains to the data of a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop: medians for normally distributed scores and mean for BLEU_score\n",
    "columns_to_drop = ['time_in_microseconds_mean', 'rouge1_mean', 'rouge2_mean', 'rougeL_mean', \n",
    "                   'METEOR_score_mean', 'Word_error_rate_mean', 'BLEU_score_mean']\n",
    "\n",
    "# Drop the specified columns from final_df\n",
    "final_df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! ðŸŽ‰\n",
    "\n",
    "Now let's clean up some of our columns to have and convert the execution time from `microseconds` to **`seconds`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time from microseconds to seconds\n",
    "final_df['time_in_seconds_median'] = final_df['time_in_microseconds_median'] / 1e6\n",
    "\n",
    "# Drop the original 'time_in_microseconds' column if you no longer need it\n",
    "final_df.drop('time_in_microseconds_median', axis=1, inplace=True)\n",
    "\n",
    "# Round all score columns to three decimal places\n",
    "score_columns = ['time_in_seconds_median', 'rouge1_median', 'rouge2_median', 'rougeL_median', 'BLEU_score_median', 'METEOR_score_median', 'Word_error_rate_median']\n",
    "for column in score_columns:\n",
    "    final_df[column] = final_df[column].round(5)\n",
    "\n",
    "# Dictionary mapping old column names to new ones\n",
    "new_column_names = {\n",
    "    'model_name': 'Model',\n",
    "    'rouge1_median': 'ROUGE-1',\n",
    "    'rouge2_median': 'ROUGE-2',\n",
    "    'rougeL_median': 'ROUGE-L',\n",
    "    'BLEU_score_median': 'BLEU',\n",
    "    'METEOR_score_median': 'METEOR',\n",
    "    'Word_error_rate_median': 'Word Error Rate',\n",
    "    'time_in_seconds_median': 'Time (s)'\n",
    "}\n",
    "\n",
    "# Rename the columns using the dictionary\n",
    "final_df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Now final_df will have the new column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're done! ðŸŽ‰\n",
    "\n",
    "Awesome!\n",
    "\n",
    "Congratulations, we now have a table that shows accuracy scores and the execution time for each model!\n",
    "You can expand this table by running this notebook (assuming you have a `modelName_results.csv` file created).\n",
    "\n",
    "Hurray!\n",
    "\n",
    "Let's get this table into `Markdown` so we can post it in our `README`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                       |   ROUGE-1 |   ROUGE-2 |   ROUGE-L |    BLEU |   METEOR |   Word Error Rate |   Time (s) |\n",
      "|:----------------------------|----------:|----------:|----------:|--------:|---------:|------------------:|-----------:|\n",
      "| blip-image-captioning-base  |   0.6     |   0.36364 |   0.57983 | 20.0762 |  0.45953 |           0.58333 |    4.16365 |\n",
      "| blip-image-captioning-large |   0.59167 |   0.33333 |   0.55844 | 19.0449 |  0.53777 |           0.72381 |   11.878   |\n",
      "| resnet-50                   |   0       |   0       |   0       |  0      |  0.03953 |           1       |    0.32517 |\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to Markdown\n",
    "markdown_table = final_df.to_markdown(index=False)\n",
    "\n",
    "# Print the Markdown table\n",
    "print(markdown_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocodataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
