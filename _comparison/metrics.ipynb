{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of the models\n",
    "\n",
    "This notebook will create a table that compares different metrics between the models that have been benchmarked inside `coco_dataset`.\n",
    "To successfully run this notebook, it is advised to have a virtual `Conda` environment so this notebook has access to the needed dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing results files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = './coco_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_results_to_dataframe(directory_path=files_path):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    # Iterate over all files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('_results.csv'):\n",
    "            # Extract the model name from the filename\n",
    "            model_name = filename.replace('_results.csv', '')\n",
    "            \n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the expected columns are present in the DataFrame\n",
    "            if set(['image_id', 'time_in_microseconds', 'prediction']).issubset(df.columns):\n",
    "                # Add the model_name column\n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                # Keep only the required columns in the specified order\n",
    "                df = df[['image_id', 'model_name', 'time_in_microseconds', 'prediction']]\n",
    "                \n",
    "                # Append the dataframe to the list\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} does not contain the required columns.\")\n",
    "    \n",
    "    # Concatenate all dataframes in the list into a single dataframe\n",
    "    results_df = pd.concat(df_list)\n",
    "    \n",
    "    # Reset the index of the resulting dataframe\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function and assign the result to a variable\n",
    "df = load_results_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information from the COCO dataset captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the captions.csv file into a DataFrame\n",
    "captions_df = pd.read_csv(os.path.join(files_path, 'captions.csv'))\n",
    "\n",
    "# Rename the 'caption' column to 'original_caption'\n",
    "captions_df.rename(columns={'caption': 'original_caption'}, inplace=True)\n",
    "\n",
    "# Merge the two DataFrames on the 'image_id' column\n",
    "df = pd.merge(df, captions_df, on='image_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "Let's make both the `predicted caption` and the `original caption` lower case and formatted the same way so the metrics that we measure are more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the text according to the specified rules\n",
    "def clean_text(text):\n",
    "    # Make the text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove any surrounding quotation marks\n",
    "    text = text.strip('\\\"')\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove the period at the end if there is one\n",
    "    if text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'original_caption' and 'prediction' columns\n",
    "df['original_caption'] = df['original_caption'].apply(clean_text)\n",
    "df['prediction'] = df['prediction'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score\n",
    "\n",
    "Among the ensemble of evaluation metrics, the `ROUGE Score` is prominent. Standing for [\"Recall-Oriented Understudy for Gisting Evaluation\"](https://en.wikipedia.org/wiki/ROUGE_(metric)), the ROUGE Score is the lynchpin of automatic text summarization.\n",
    "\n",
    "The `ROUGE Score` has three main components: **`ROUGE-N`**, **`ROUGE-L`**, and **`ROUGE-S`**. \n",
    "\n",
    "Each ROUGE score component offers a different perspective on the quality of the system-generated summary, considering different aspects of language and sentence structure. This is why a combination of these measures is usually used in evaluating system outputs in NLP tasks.\n",
    "\n",
    "\n",
    "#### ROUGE-N\n",
    "`ROUGE-N` is a component of the ROUGE score that quantifies the overlap of [N-grams](https://en.wikipedia.org/wiki/N-gram) (contiguous sequences of N items - typically words or characters) between the system-generated summary and the reference summary. It provides insights into the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the system's output by considering the matching N-gram sequences.\n",
    "\n",
    "`ROUGE-N` essentially refers to the overlap of`n-grams`. It consists of `ROUGE-1` (overlap of **unigrams** - each word - between the system and reference summaries) and `ROUGE-2` (refers to the overlap of **bigrams** between the system and reference summaries).\n",
    "\n",
    "#### ROUGE-L \n",
    "`ROUGE-L`, another component of the `ROUGE Score`, calculates the [Longest Common Subsequence (LCS)](https://en.wikipedia.org/wiki/Longest_common_subsequence) between the system and reference summaries. Unlike N-grams, LCS measures the maximum sequence of words (not necessarily contiguous) that appear in both summaries. It offers a more flexible similarity measure and helps capture shared information beyond strict word-for-word matches.\n",
    "\n",
    "#### ROUGE-S\n",
    "`ROUGE-S` focuses on [skip-bigrams](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c). A skip-bigram is a pair of words in a sentence that allows for gaps or words in between. This component identifies the skip-bigram overlap between the system and reference summaries, enabling the assessment of sentence-level structure similarity. It can capture paraphrasing relationships between sentences and provide insights into the system's ability to convey information with flexible word ordering.\n",
    "\n",
    "> the text above was taken from https://thepythoncode.com/article/calculate-rouge-score-in-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `ROUGE Score` in our `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Function to calculate ROUGE scores for a single row\n",
    "def calculate_rouge_scores(row):\n",
    "    # Initialize the scorer for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores = scorer.score(row['original_caption'], row['prediction'])\n",
    "    \n",
    "    # Extract and return the scores\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "# Apply the calculate_rouge_scores function to each row in df\n",
    "# The result will be a new DataFrame with the ROUGE scores\n",
    "rouge_scores_df = df.apply(calculate_rouge_scores, axis=1, result_type='expand')\n",
    "\n",
    "# Concatenate the original df with the new DataFrame containing the ROUGE scores\n",
    "df = pd.concat([df, rouge_scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above will create three new columns `rouge1`, `rouge2` and `rougeL`. \n",
    "\n",
    "`rouge-scorer` returns a `Score` object with three different parameters. For example:\n",
    "\n",
    "```python\n",
    "rouge1:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.7142857142857143, recall=0.8333333333333334, fmeasure=0.7692307692307692)]\n",
    "rouge2:\n",
    "[Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272), Score(precision=0.3333333333333333, recall=0.4, fmeasure=0.3636363636363636)]\n",
    "rougeL:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)]\n",
    "```\n",
    "\n",
    "The choice between using `fmeasure`, `precision`, or `recall` depends on what aspect of the summary's quality you want to emphasize:\n",
    "\n",
    "- **Precision** (specificity) measures the fraction of relevant instances among the retrieved instances. In the context of ROUGE, it calculates how many of the words in the predicted summary (generated caption) are also found in the reference summary (original caption).\n",
    "\n",
    "- **Recall** measures the fraction of relevant instances that were retrieved. In the context of ROUGE, it calculates how much of the reference summary is captured by the predicted summary.\n",
    "\n",
    "- **F-measure**(or F1 score) is the harmonic mean of precision and recall. It provides a single score that balances both precision and recall. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "We are choosing the `F-measure` because it strikes a nice balance between `precision` and `recall`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU \n",
    "\n",
    "`BLEU` stands for [\"Bilingual Evaluation Understudy\"](https://en.wikipedia.org/wiki/BLEU) and it's a metric commonly used in NLP. IIt acn be used in text summarization, paraphrasing tasks and inclusively *image captioning*.\n",
    "\n",
    "The BLEU score is based on a simple idea, comparing the machine-generated translations with human-generated translations that are considered correct.\n",
    "Here's how it works:\n",
    "\n",
    "- The machine translation system generates translations for a set of sentences.\n",
    "- These machine-generated translations are compared to the reference translations.\n",
    "- The comparison is done by counting how many words or phrases from the machine-generated translations match the words or phrases in the reference translations.\n",
    "- The more matches there are, the higher the BLEU score will be.\n",
    "\n",
    "The BLEU score considers the precision of matching words or phrases. It also considers the length of the translations to avoid favoring shorter translations that may have an advantage in matching words by chance.\n",
    "\n",
    "The BLEU score is typically represented as a value between 0 and 1, with 1 being a perfect match and 0 being a perfect mismatch to the reference translations.\n",
    "\n",
    "> the text above was taken from https://thepythoncode.com/article/bleu-score-in-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `BLUE` score in our `df` dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocodataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
