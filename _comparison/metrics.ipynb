{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of the models\n",
    "\n",
    "This notebook will create a table that compares different metrics between the models that have been benchmarked inside `coco_dataset`.\n",
    "To successfully run this notebook, it is advised to have a virtual `Conda` environment so this notebook has access to the needed dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing results files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = './coco_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_results_to_dataframe(directory_path=files_path):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    # Iterate over all files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('_results.csv'):\n",
    "            # Extract the model name from the filename\n",
    "            model_name = filename.replace('_results.csv', '')\n",
    "            \n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Read the CSV file into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the expected columns are present in the DataFrame\n",
    "            if set(['image_id', 'time_in_microseconds', 'prediction']).issubset(df.columns):\n",
    "                # Add the model_name column\n",
    "                df['model_name'] = model_name\n",
    "                \n",
    "                # Keep only the required columns in the specified order\n",
    "                df = df[['image_id', 'model_name', 'time_in_microseconds', 'prediction']]\n",
    "                \n",
    "                # Append the dataframe to the list\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} does not contain the required columns.\")\n",
    "    \n",
    "    # Concatenate all dataframes in the list into a single dataframe\n",
    "    results_df = pd.concat(df_list)\n",
    "    \n",
    "    # Reset the index of the resulting dataframe\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Call the function and assign the result to a variable\n",
    "df = load_results_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information from the COCO dataset captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the captions.csv file into a DataFrame\n",
    "captions_df = pd.read_csv(os.path.join(files_path, 'captions.csv'))\n",
    "\n",
    "# Rename the 'caption' column to 'original_caption'\n",
    "captions_df.rename(columns={'caption': 'original_caption'}, inplace=True)\n",
    "\n",
    "# Merge the two DataFrames on the 'image_id' column\n",
    "df = pd.merge(df, captions_df, on='image_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "Let's make both the `predicted caption` and the `original caption` lower case and formatted the same way so the metrics that we measure are more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the text according to the specified rules\n",
    "def clean_text(text):\n",
    "    # Make the text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove any surrounding quotation marks\n",
    "    text = text.strip('\\\"')\n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    # Remove the period at the end if there is one\n",
    "    if text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'original_caption' and 'prediction' columns\n",
    "df['original_caption'] = df['original_caption'].apply(clean_text)\n",
    "df['prediction'] = df['prediction'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score\n",
    "\n",
    "Among the ensemble of evaluation metrics, the `ROUGE Score` is prominent. Standing for [\"Recall-Oriented Understudy for Gisting Evaluation\"](https://en.wikipedia.org/wiki/ROUGE_(metric)), the ROUGE Score is the lynchpin of automatic text summarization.\n",
    "\n",
    "The `ROUGE Score` has three main components: **`ROUGE-N`**, **`ROUGE-L`**, and **`ROUGE-S`**. \n",
    "\n",
    "Each ROUGE score component offers a different perspective on the quality of the system-generated summary, considering different aspects of language and sentence structure. This is why a combination of these measures is usually used in evaluating system outputs in NLP tasks.\n",
    "\n",
    "\n",
    "#### ROUGE-N\n",
    "`ROUGE-N` is a component of the ROUGE score that quantifies the overlap of [N-grams](https://en.wikipedia.org/wiki/N-gram) (contiguous sequences of N items - typically words or characters) between the system-generated summary and the reference summary. It provides insights into the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the system's output by considering the matching N-gram sequences.\n",
    "\n",
    "#### ROUGE-L \n",
    "`ROUGE-L`, another component of the `ROUGE Score`, calculates the [Longest Common Subsequence (LCS)](https://en.wikipedia.org/wiki/Longest_common_subsequence) between the system and reference summaries. Unlike N-grams, LCS measures the maximum sequence of words (not necessarily contiguous) that appear in both summaries. It offers a more flexible similarity measure and helps capture shared information beyond strict word-for-word matches.\n",
    "\n",
    "#### ROUGE-S\n",
    "`ROUGE-S` focuses on [skip-bigrams](https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c). A skip-bigram is a pair of words in a sentence that allows for gaps or words in between. This component identifies the skip-bigram overlap between the system and reference summaries, enabling the assessment of sentence-level structure similarity. It can capture paraphrasing relationships between sentences and provide insights into the system's ability to convey information with flexible word ordering.\n",
    "\n",
    "> the text above was taken from https://thepythoncode.com/article/calculate-rouge-score-in-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now focus on adding the `ROUGE Score` in our `df` dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocodataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
